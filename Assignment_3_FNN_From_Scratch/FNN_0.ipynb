{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16879/3879243728.py:363: DeprecationWarning: The parameter \"txt\" has been renamed to \"text\" in 2.7.6\n",
      "  pdf.cell(0, 10, txt=log, ln=True)\n",
      "/tmp/ipykernel_16879/3879243728.py:363: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=True use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
      "  pdf.cell(0, 10, txt=log, ln=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.1372, Test Accuracy: 0.4661, Test Macro-F1: 0.2523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16879/3879243728.py:363: DeprecationWarning: The parameter \"txt\" has been renamed to \"text\" in 2.7.6\n",
      "  pdf.cell(0, 10, txt=log, ln=True)\n",
      "/tmp/ipykernel_16879/3879243728.py:363: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=True use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
      "  pdf.cell(0, 10, txt=log, ln=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9973, Test Accuracy: 0.4993, Test Macro-F1: 0.2760\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 419\u001b[0m\n\u001b[1;32m    416\u001b[0m nn \u001b[38;5;241m=\u001b[39m NeuralNetwork(arch, learning_rate\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Train the network\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m train_losses, val_losses, train_accuracies, val_accuracies, val_macro_f1_scores \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\n\u001b[1;32m    421\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# Collect metrics\u001b[39;00m\n\u001b[1;32m    424\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: lr,\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitecture\u001b[39m\u001b[38;5;124m'\u001b[39m: arch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnn_model\u001b[39m\u001b[38;5;124m'\u001b[39m: nn  \u001b[38;5;66;03m# Save the model for generating confusion matrix later\u001b[39;00m\n\u001b[1;32m    433\u001b[0m })\n",
      "Cell \u001b[0;32mIn[10], line 312\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, x_train, y_train, x_val, y_val, epochs, batch_size)\u001b[0m\n\u001b[1;32m    310\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x_batch, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    311\u001b[0m     loss_grad \u001b[38;5;241m=\u001b[39m output \u001b[38;5;241m-\u001b[39m y_batch\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# Compute metrics for training set\u001b[39;00m\n\u001b[1;32m    315\u001b[0m train_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x_train)\n",
      "Cell \u001b[0;32mIn[10], line 292\u001b[0m, in \u001b[0;36mNeuralNetwork.backward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# If layer is Dense, pass learning rate to the backward method\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, (Dense, BatchNormalization)):\n\u001b[0;32m--> 292\u001b[0m         grad_output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m         grad_output \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mbackward(grad_output)\n",
      "Cell \u001b[0;32mIn[10], line 150\u001b[0m, in \u001b[0;36mDense.backward\u001b[0;34m(self, grad_output, learning_rate)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, grad_output, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m learning_rate:\n\u001b[0;32m--> 150\u001b[0m         grad_input \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m         grad_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput\u001b[38;5;241m.\u001b[39mT, grad_output)\n\u001b[1;32m    152\u001b[0m         grad_biases \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(grad_output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf  # Used here only to load the FashionMNIST dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from fpdf import FPDF\n",
    "import pickle\n",
    "\n",
    "\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font(family=\"Times\",style=\"b\", size=10)\n",
    "\n",
    "\n",
    "# --- Load and Preprocess FashionMNIST ---\n",
    "# def load_fashion_mnist():\n",
    "#     (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "#     # Normalize images to the range [0, 1]\n",
    "#     train_images = train_images / 255.0\n",
    "#     test_images = test_images / 255.0\n",
    "\n",
    "#     # Flatten images from (28, 28) to (784,) for the fully connected network\n",
    "#     train_images = train_images.reshape(-1, 784)\n",
    "#     test_images = test_images.reshape(-1, 784)\n",
    "\n",
    "#     # Convert labels to one-hot encoding\n",
    "#     train_labels = np.eye(10)[train_labels]\n",
    "#     test_labels = np.eye(10)[test_labels]\n",
    "\n",
    "#     return (train_images, train_labels), (test_images, test_labels)# Function to convert labels to one-hot encoding\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "# # Function to load and preprocess FashionMNIST with a validation split\n",
    "# def load_fashion_mnist(batch_size=64, val_split=0.1):\n",
    "#     # Define transformations: Convert to tensor and normalize images to [0, 1]\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.5,), (0.5,))  # Normalizing to [-1, 1] range\n",
    "#     ])\n",
    "    \n",
    "#     # Load datasets\n",
    "#     full_train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "#     # test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "#     with open('b1.pkl', 'rb') as b1:\n",
    "#         test_dataset = pickle.load(b1)\n",
    "    \n",
    "#     # Split train dataset into train and validation\n",
    "#     val_size = int(len(full_train_dataset) * val_split)\n",
    "#     train_size = len(full_train_dataset) - val_size\n",
    "#     train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "    \n",
    "#     # Load data into DataLoader for batching\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "#     # Convert to numpy arrays for custom training loop\n",
    "#     train_images = train_dataset.dataset.data[train_dataset.indices].numpy().reshape(-1, 784) / 255.0\n",
    "#     val_images = val_dataset.dataset.data[val_dataset.indices].numpy().reshape(-1, 784) / 255.0\n",
    "#     test_images = test_dataset.data.numpy().reshape(-1, 784) / 255.0\n",
    "    \n",
    "#     # One-hot encode labels\n",
    "#     train_labels = one_hot_encode(train_dataset.dataset.targets[train_dataset.indices].numpy())\n",
    "#     val_labels = one_hot_encode(val_dataset.dataset.targets[val_dataset.indices].numpy())\n",
    "#     test_labels = one_hot_encode(test_dataset.targets.numpy())\n",
    "    \n",
    "#     return (train_images, train_labels), (val_images, val_labels), (test_images, test_labels), train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def load_fashion_mnist(batch_size=64, val_split=0.1):\n",
    "    # Define transformations: Convert to tensor and normalize images to [0, 1]\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalizing to [-1, 1] range\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    full_train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    with open('b1.pkl', 'rb') as b1:\n",
    "        test_dataset = pickle.load(b1)  # Assuming test_dataset is a TensorDataset\n",
    "\n",
    "    # Split train dataset into train and validation\n",
    "    val_size = int(len(full_train_dataset) * val_split)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Load data into DataLoader for batching\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Convert to numpy arrays for custom training loop\n",
    "    train_images = train_dataset.dataset.data[train_dataset.indices].numpy().reshape(-1, 784) / 255.0\n",
    "    val_images = val_dataset.dataset.data[val_dataset.indices].numpy().reshape(-1, 784) / 255.0\n",
    "    \n",
    "    # For test_dataset, assume the first element of each item is the image and the second is the label\n",
    "    test_images = test_dataset.tensors[0].numpy().reshape(-1, 784) / 255.0\n",
    "    test_labels = one_hot_encode(test_dataset.tensors[1].numpy())\n",
    "    \n",
    "    # One-hot encode labels for training and validation sets\n",
    "    train_labels = one_hot_encode(train_dataset.dataset.targets[train_dataset.indices].numpy())\n",
    "    val_labels = one_hot_encode(val_dataset.dataset.targets[val_dataset.indices].numpy())\n",
    "    \n",
    "    return (train_images, train_labels), (val_images, val_labels), (test_images, test_labels), train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# --- Helper functions ---\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# --- Dense Layer ---\n",
    "class Dense:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.dot(x, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, grad_output, learning_rate=None):\n",
    "        if learning_rate:\n",
    "            grad_input = np.dot(grad_output, self.weights.T)\n",
    "            grad_weights = np.dot(self.input.T, grad_output)\n",
    "            grad_biases = np.sum(grad_output, axis=0, keepdims=True)\n",
    "\n",
    "            self.weights -= learning_rate * grad_weights\n",
    "            self.biases -= learning_rate * grad_biases\n",
    "        return grad_input\n",
    "\n",
    "# --- Batch Normalization ---\n",
    "class BatchNormalization:\n",
    "    def __init__(self, num_features, epsilon=1e-5, momentum=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = np.ones((1, num_features))\n",
    "        self.beta = np.zeros((1, num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mean = np.mean(x, axis=0)\n",
    "        self.variance = np.var(x, axis=0)\n",
    "        self.x_normalized = (x - self.mean) / np.sqrt(self.variance + self.epsilon)\n",
    "        return self.gamma * self.x_normalized + self.beta\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_gamma = np.sum(grad_output * self.x_normalized, axis=0, keepdims=True)\n",
    "        grad_beta = np.sum(grad_output, axis=0, keepdims=True)\n",
    "\n",
    "        self.gamma -= learning_rate * grad_gamma\n",
    "        self.beta -= learning_rate * grad_beta\n",
    "\n",
    "        return grad_output\n",
    "\n",
    "# --- Activation Layer (ReLU) ---\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return relu(x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * relu_derivative(self.input)\n",
    "\n",
    "# --- Dropout Layer ---\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            self.mask = (np.random.rand(*x.shape) > self.dropout_rate).astype(float)\n",
    "            return x * self.mask\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.mask\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Apply softmax function\n",
    "        exps = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Compute gradient on softmax output\n",
    "        self.dinputs = dvalues  # Gradient is passed from the loss gradient\n",
    "        return self.dinputs\n",
    "\n",
    "\n",
    "# --- Adam Optimizer ---\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, layer, grad_w, grad_b):\n",
    "        self.t += 1\n",
    "\n",
    "        if layer not in self.m:\n",
    "            self.m[layer] = {\"w\": np.zeros_like(grad_w), \"b\": np.zeros_like(grad_b)}\n",
    "            self.v[layer] = {\"w\": np.zeros_like(grad_w), \"b\": np.zeros_like(grad_b)}\n",
    "\n",
    "        # Update biased first moment estimate\n",
    "        self.m[layer][\"w\"] = self.beta1 * self.m[layer][\"w\"] + (1 - self.beta1) * grad_w\n",
    "        self.m[layer][\"b\"] = self.beta1 * self.m[layer][\"b\"] + (1 - self.beta1) * grad_b\n",
    "\n",
    "        # Update biased second raw moment estimate\n",
    "        self.v[layer][\"w\"] = self.beta2 * self.v[layer][\"w\"] + (1 - self.beta2) * (grad_w ** 2)\n",
    "        self.v[layer][\"b\"] = self.beta2 * self.v[layer][\"b\"] + (1 - self.beta2) * (grad_b ** 2)\n",
    "\n",
    "        # Correct bias in first moment\n",
    "        m_w_hat = self.m[layer][\"w\"] / (1 - self.beta1 ** self.t)\n",
    "        m_b_hat = self.m[layer][\"b\"] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "        # Correct bias in second moment\n",
    "        v_w_hat = self.v[layer][\"w\"] / (1 - self.beta2 ** self.t)\n",
    "        v_b_hat = self.v[layer][\"b\"] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "        # Update weights and biases\n",
    "        layer.weights -= self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "        layer.biases -= self.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Helper function for accuracy\n",
    "def compute_accuracy(y_pred, y_true):\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_true, axis=1)\n",
    "    return accuracy_score(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Helper function for macro-F1 score\n",
    "def compute_macro_f1(y_pred, y_true):\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_true, axis=1)\n",
    "    return f1_score(y_true_classes, y_pred_classes, average='macro')\n",
    "\n",
    "# Helper function for computing cross-entropy loss\n",
    "def compute_loss(y_pred, y_true):\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-12), axis=1))\n",
    "\n",
    "\n",
    "\n",
    "# --- Neural Network ---\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.001):\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x) if not isinstance(layer, Dropout) else layer.forward(x, training)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        for layer in reversed(self.layers):\n",
    "            # If layer is Dense, pass learning rate to the backward method\n",
    "            if isinstance(layer, (Dense, BatchNormalization)):\n",
    "                grad_output = layer.backward(grad_output, self.learning_rate)\n",
    "            else:\n",
    "                grad_output = layer.backward(grad_output)\n",
    "\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs, batch_size):\n",
    "        train_losses, val_losses = [], []\n",
    "        train_accuracies, val_accuracies = [], []\n",
    "        val_macro_f1_scores = []\n",
    "\n",
    "        \n",
    "\n",
    "        training_logs = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Mini-batch training\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch, y_batch = x_train[i:i + batch_size], y_train[i:i + batch_size]\n",
    "                output = self.forward(x_batch, training=True)\n",
    "                loss_grad = output - y_batch\n",
    "                self.backward(loss_grad)\n",
    "            \n",
    "            # Compute metrics for training set\n",
    "            train_pred = self.predict(x_train)\n",
    "            train_loss = compute_loss(train_pred, y_train)\n",
    "            train_acc = compute_accuracy(train_pred, y_train)\n",
    "            \n",
    "            # Compute metrics for validation set\n",
    "            val_pred = self.predict(x_val)\n",
    "            val_loss = compute_loss(val_pred, y_val)\n",
    "            val_acc = compute_accuracy(val_pred, y_val)\n",
    "            val_macro_f1 = compute_macro_f1(val_pred, y_val)\n",
    "\n",
    "            # Append metrics\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_accuracies.append(val_acc)\n",
    "            val_macro_f1_scores.append(val_macro_f1)\n",
    "\n",
    "\n",
    "            # # Log training details to a file\n",
    "            # log_file = open(\"training_log.txt\", \"a\")\n",
    "\n",
    "            # # Inside your training loop\n",
    "            # log_file.write(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "            #             f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "            #             f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "            #             f\"Val Macro-F1: {val_macro_f1:.4f}\\n\")\n",
    "            \n",
    "            training_logs.append(\n",
    "                f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "                f\"Val Macro-F1: {val_macro_f1:.4f}\\n\"\n",
    "            )\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            # Print epoch summary\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val Macro-F1: {val_macro_f1:.4f}\")\n",
    "            \n",
    "            # log_file.close()\n",
    "\n",
    "        # Save training logs to a PDF file\n",
    "        for log in training_logs:\n",
    "            pdf.cell(0, 10, txt=log, ln=True)\n",
    "\n",
    "\n",
    "        return train_losses, val_losses, train_accuracies, val_accuracies, val_macro_f1_scores\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(x, training=False)\n",
    "    \n",
    "\n",
    "def plot_metrics(metrics, label, title, ylabel, filename):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    for metric_data in metrics:\n",
    "        plt.plot(metric_data['epochs'], metric_data['values'], label=f\"LR: {metric_data['learning_rate']} - Arch:\")\n",
    "    # plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{filename}.png\")  # Save plot as a PNG file for LaTeX\n",
    "    plt.show()\n",
    "\n",
    "# --- Load and Preprocess FashionMNIST ---\n",
    "# --- Setup for FashionMNIST Training ---\n",
    "(train_images, train_labels), (val_images, val_labels), (test_images, test_labels), train_loader, val_loader, test_loader = load_fashion_mnist()\n",
    "\n",
    "# --- Example Setup for a Network ---\n",
    "# nn = NeuralNetwork([\n",
    "#     Dense(input_dim=784, output_dim=128),\n",
    "#     BatchNormalization(num_features=128),\n",
    "#     ReLU(),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(input_dim=128, output_dim=64),\n",
    "#     BatchNormalization(num_features=64),\n",
    "#     ReLU(),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(input_dim=64, output_dim=10),\n",
    "#     Softmax()\n",
    "# ])\n",
    "\n",
    "learning_rates = [0.005, 0.001, 0.0005, 0.0001]\n",
    "architectures = [\n",
    "    [Dense(784, 128), BatchNormalization(128), ReLU(), Dense(128, 64), BatchNormalization(64), ReLU(), Dense(64, 10), Softmax()],\n",
    "    [Dense(784, 256), BatchNormalization(256), ReLU(), Dense(256, 128), BatchNormalization(128), ReLU(), Dense(128, 10), Softmax()],\n",
    "    [Dense(784, 512), BatchNormalization(512), ReLU(), Dense(512, 256), BatchNormalization(256), ReLU(), Dense(256, 10), Softmax()]\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_macro_f1 = 0\n",
    "best_arch = None    \n",
    "best_lr = None\n",
    "\n",
    "results = []\n",
    "for lr in learning_rates:\n",
    "    for arch in architectures:\n",
    "        nn = NeuralNetwork(arch, learning_rate=lr)\n",
    "        \n",
    "        # Train the network\n",
    "        train_losses, val_losses, train_accuracies, val_accuracies, val_macro_f1_scores = nn.train(\n",
    "            train_images, train_labels, val_images, val_labels, epochs=10, batch_size=64\n",
    "        )\n",
    "\n",
    "        # Collect metrics\n",
    "        results.append({\n",
    "            'learning_rate': lr,\n",
    "            'architecture': arch,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'val_macro_f1_scores': val_macro_f1_scores,\n",
    "            'nn_model': nn  # Save the model for generating confusion matrix later\n",
    "        })\n",
    "        \n",
    "        # Track the best model based on validation macro-F1 score\n",
    "        max_f1 = max(val_macro_f1_scores)\n",
    "        if max_f1 > best_macro_f1:\n",
    "            best_macro_f1 = max_f1\n",
    "            best_model = nn\n",
    "            best_arch = arch\n",
    "            best_lr = lr\n",
    "\n",
    "            \n",
    "            \n",
    "        # Plot training curves\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label=\"Train Loss\")\n",
    "        plt.plot(val_losses, label=\"Validation Loss\")\n",
    "        plt.title(f\"Learning Rate {lr} - Loss\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "        plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "        plt.title(f\"Learning Rate {lr} - Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_count = 0\n",
    "\n",
    "# Prepare data for plotting\n",
    "for result in results:\n",
    "    lr = result['learning_rate']\n",
    "    arch = result['architecture']\n",
    "    epochs = range(1, len(result['train_losses']) + 1)\n",
    "    \n",
    "    # Plot Loss\n",
    "    plot_metrics([{'epochs': epochs, 'values': result['train_losses'], 'learning_rate': lr, 'arch': str(arch)}], \n",
    "                 'Training Loss', f\"Loss vs Epoch (LR: {lr}, Arch: {arch})\", \"Loss\", f\"train_loss_plot_{plot_count}\")\n",
    "    \n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plot_metrics([{'epochs': epochs, 'values': result['train_accuracies'], 'learning_rate': lr, 'arch': str(arch)}], \n",
    "                 'Training Accuracy', f\"Accuracy vs Epoch (LR: {lr}, Arch: {arch})\", \"Accuracy\", f\"train_accuracy_plot_{plot_count}\")\n",
    "    \n",
    "    # Plot F1 Score\n",
    "    plot_metrics([{'epochs': epochs, 'values': result['val_macro_f1_scores'], 'learning_rate': lr, 'arch': str(arch)}], \n",
    "                 'Validation Macro F1 Score', f\"F1 Score vs Epoch (LR: {lr}, Arch: {arch})\", \"F1 Score\", f\"val_f1_score_plot_{plot_count}\")\n",
    "\n",
    "\n",
    "\n",
    "confusion_plot_count = 0\n",
    "\n",
    "for result in results:\n",
    "    nn = result['nn_model']\n",
    "    lr = result['learning_rate']\n",
    "    arch = result['architecture']\n",
    "    \n",
    "    # Generate predictions and confusion matrix for the validation set\n",
    "    val_predictions = nn.predict(val_images)\n",
    "    val_predictions = np.argmax(val_predictions, axis=1)  # Get the predicted class\n",
    "    y_true = np.argmax(val_labels, axis=1)  # Assuming one-hot encoded labels\n",
    "\n",
    "    cm = confusion_matrix(y_true, val_predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.title(f\"Confusion Matrix (LR: {lr}, Arch)\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.savefig(f\"confusion_matrix_{confusion_plot_count}.png\")\n",
    "    plt.show()\n",
    "\n",
    "    pdf.add_page()\n",
    "    pdf.image(f\"confusion_matrix_{confusion_plot_count}.png\", x=10, y=10, w=180)\n",
    "    confusion_plot_count += 1\n",
    "\n",
    "\n",
    "# Get predictions from the best model on the validation set\n",
    "val_pred = best_model.predict(val_images)\n",
    "val_pred_classes = np.argmax(val_pred, axis=1)\n",
    "val_true_classes = np.argmax(val_labels, axis=1)\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(val_true_classes, val_pred_classes)\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix for Best Model on Validation Set\")\n",
    "plt.savefig(\"confusion_matrix_best_model.png\")\n",
    "plt.show()\n",
    "\n",
    "pdf.add_page()\n",
    "pdf.image(\"confusion_matrix_best_model.png\", x=10, y=10, w=180)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_pred = best_model.predict(test_images)\n",
    "test_loss = compute_loss(test_pred, test_labels)\n",
    "test_acc = compute_accuracy(test_pred, test_labels)\n",
    "test_macro_f1 = compute_macro_f1(test_pred, test_labels)\n",
    "\n",
    "test_results = f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Macro-F1: {test_macro_f1:.4f}\"\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Macro-F1: {test_macro_f1:.4f}\")\n",
    "\n",
    "# Add a separator line before test results\n",
    "pdf.add_page()\n",
    "pdf.cell(0, 10, \"\", ln=True)  # Blank line\n",
    "pdf.cell(0, 10, \"Test Results:\", ln=True)  # Heading for the test results\n",
    "pdf.cell(0, 10, test_results, ln=True)  # Add test results to the PDF\n",
    "\n",
    "pdf.output(\"training_report.pdf\")  # Save the PDF file\n",
    "print(\"Training details, test results, and confusion matrices saved to training_report.pdf.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 ... 8 1 5]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def clear_intermediate_outputs(model):\n",
    "    \"\"\"\n",
    "    Clears the intermediate input and output data from each layer to save space.\n",
    "    \"\"\"\n",
    "    for layer in model.layers:\n",
    "        # Check if the layer has attributes for intermediate values, and clear them\n",
    "        if hasattr(layer, 'input'):\n",
    "            layer.input = None\n",
    "        if hasattr(layer, 'output'):\n",
    "            layer.output = None\n",
    "        # If there are additional large attributes, clear them similarly:\n",
    "        if hasattr(layer, 'x_normalized'):  # For BatchNormalization layers\n",
    "            layer.x_normalized = None\n",
    "        if hasattr(layer, 'mean'):\n",
    "            layer.mean = None\n",
    "        if hasattr(layer, 'variance'):\n",
    "            layer.variance = None\n",
    "\n",
    "def predict_with_loaded_model(model, query_images):\n",
    "    predictions = model.predict(query_images)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    return predicted_classes\n",
    "\n",
    "\n",
    "# Clear intermediate data from the model\n",
    "clear_intermediate_outputs(best_model)\n",
    "\n",
    "# Save the best model to disk\n",
    "with open(\"best_model_nn.pkl\", \"wb\") as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Load the best model from disk\n",
    "with open(\"best_model_nn.pkl\", \"rb\") as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "# Predict with the loaded model\n",
    "query_images = test_images\n",
    "predicted_classes = predict_with_loaded_model(loaded_model, query_images)\n",
    "\n",
    "print(predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 ... 8 1 5]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def predict_with_loaded_model(model, query_images):\n",
    "    predictions = model.predict(query_images)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    return predicted_classes\n",
    "\n",
    "# Save the best model to disk\n",
    "with open(\"best_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Load the best model from disk\n",
    "with open(\"best_model.pkl\", \"rb\") as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "# Predict with the loaded model\n",
    "query_images = test_images\n",
    "predicted_classes = predict_with_loaded_model(loaded_model, query_images)\n",
    "\n",
    "print(predicted_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 ... 8 1 5]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Save only the model weights (gamma, beta, etc.) to a file\n",
    "def save_model_weights(model, filename=\"best_model_weights.pkl\"):\n",
    "    weights = {}\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, 'gamma') and hasattr(layer, 'beta'):\n",
    "            weights[f\"layer_{i}_gamma\"] = layer.gamma\n",
    "            weights[f\"layer_{i}_beta\"] = layer.beta\n",
    "        if hasattr(layer, 'weights'):\n",
    "            weights[f\"layer_{i}_weights\"] = layer.weights\n",
    "            weights[f\"layer_{i}_biases\"] = layer.biases\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(weights, file)\n",
    "\n",
    "save_model_weights(best_model)\n",
    "\n",
    "\n",
    "\n",
    "# Load weights into a new model instance\n",
    "def load_model_weights(model, filename=\"best_model_weights.pkl\"):\n",
    "    with open(filename, \"rb\") as file:\n",
    "        weights = pickle.load(file)\n",
    "    \n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, 'gamma') and hasattr(layer, 'beta'):\n",
    "            layer.gamma = weights[f\"layer_{i}_gamma\"]\n",
    "            layer.beta = weights[f\"layer_{i}_beta\"]\n",
    "        if hasattr(layer, 'weights'):\n",
    "            layer.weights = weights[f\"layer_{i}_weights\"]\n",
    "            layer.biases = weights[f\"layer_{i}_biases\"]\n",
    "\n",
    "\n",
    "def predict_with_loaded_model(model, query_images):\n",
    "    predictions = model.predict(query_images)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    return predicted_classes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test Block (for loading and using the model)\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the trained model weights\n",
    "    nn = NeuralNetwork(best_arch, best_lr)  # Ensure to create the same architecture\n",
    "    load_model_weights(nn, 'best_model_weights.pkl')  # Load the saved weights\n",
    "\n",
    "    # Assuming you have test images to classify\n",
    "    query_images = test_images  # Replace with actual query images\n",
    "\n",
    "    # Predict labels for the query images\n",
    "    predicted_labels = predict_with_loaded_model(nn, query_images)\n",
    "\n",
    "    # Print or return predicted labels\n",
    "    print(predicted_labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
